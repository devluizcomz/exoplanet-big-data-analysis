# ğŸ—ï¸ Architecture Documentation

## System Architecture Overview

This document describes the technical architecture of the Exoplanet Big Data Analysis project.

---

## ğŸ“Š Data Flow Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA INGESTION LAYER                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  NASA Exoplanet Archive API                                â”‚
â”‚  â†“                                                          â”‚
â”‚  Python Download Script (01_download_data.py)              â”‚
â”‚  â†“                                                          â”‚
â”‚  Local Storage (data/raw/exoplanets_raw.csv)               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DISTRIBUTED STORAGE LAYER                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  HDFS (Hadoop Distributed File System)                     â”‚
â”‚  â”œâ”€â”€ /user/exoplanets/raw/                                 â”‚
â”‚  â””â”€â”€ /user/exoplanets/processed/                           â”‚
â”‚                                                             â”‚
â”‚  Block Size: 128MB                                         â”‚
â”‚  Replication Factor: 3                                     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PROCESSING & COMPUTE LAYER                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Apache Spark 3.x                                          â”‚
â”‚  â”œâ”€â”€ Spark Core: Distributed processing                    â”‚
â”‚  â”œâ”€â”€ Spark SQL: Structured data processing                 â”‚
â”‚  â””â”€â”€ DataFrame API: High-level transformations             â”‚
â”‚                                                             â”‚
â”‚  Processing Script: 02_spark_analysis.py                   â”‚
â”‚                                                             â”‚
â”‚  Key Operations:                                           â”‚
â”‚  â€¢ Data cleaning & validation                              â”‚
â”‚  â€¢ Feature engineering (habitability score)                â”‚
â”‚  â€¢ Aggregations & statistics                               â”‚
â”‚  â€¢ Parquet format conversion                               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA WAREHOUSE LAYER                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Apache Hive                                               â”‚
â”‚  â”œâ”€â”€ External Tables                                       â”‚
â”‚  â”œâ”€â”€ Parquet Storage Format                               â”‚
â”‚  â””â”€â”€ SQL Query Interface                                   â”‚
â”‚                                                             â”‚
â”‚  Tables:                                                   â”‚
â”‚  â€¢ exoplanets_enriched                                     â”‚
â”‚  â€¢ earth_candidates                                        â”‚
â”‚  â€¢ discovery_stats                                         â”‚
â”‚  â€¢ category_stats                                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ANALYSIS & INSIGHTS LAYER                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  SQL Queries (analysis_queries.sql)                        â”‚
â”‚  Python Visualizations (03_visualizations.py)              â”‚
â”‚  â†“                                                          â”‚
â”‚  Insights & Reports                                        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ³ Docker Infrastructure

### Container Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker Network                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  NameNode      â”‚  â”‚  DataNode      â”‚               â”‚
â”‚  â”‚  (HDFS Master) â”‚  â”‚  (HDFS Worker) â”‚               â”‚
â”‚  â”‚  Port: 9870    â”‚  â”‚                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Spark Master   â”‚  â”‚ Spark Worker   â”‚               â”‚
â”‚  â”‚ Port: 8080     â”‚  â”‚                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚  â”‚  Hive Server   â”‚                                     â”‚
â”‚  â”‚  Port: 10000   â”‚                                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Container Specifications

| Container | Image | Ports | Resources |
|-----------|-------|-------|-----------|
| NameNode | bde2020/hadoop-namenode:2.0.0 | 9870, 9000 | 1GB RAM |
| DataNode | bde2020/hadoop-datanode:2.0.0 | - | 2GB RAM |
| Spark Master | bde2020/spark-master:3.1.1 | 8080, 7077 | 1GB RAM |
| Spark Worker | bde2020/spark-worker:3.1.1 | - | 2GB RAM |
| Hive Server | bde2020/hive:2.3.2 | 10000 | 2GB RAM |

---

## ğŸ“¦ Data Model

### Raw Data Schema

```
exoplanets_raw.csv
â”œâ”€â”€ pl_name: STRING          # Planet name
â”œâ”€â”€ hostname: STRING         # Host star name
â”œâ”€â”€ discoverymethod: STRING  # Detection method
â”œâ”€â”€ disc_year: INT          # Discovery year
â”œâ”€â”€ pl_orbper: DOUBLE       # Orbital period (days)
â”œâ”€â”€ pl_rade: DOUBLE         # Planet radius (Earth radii)
â”œâ”€â”€ pl_bmasse: DOUBLE       # Planet mass (Earth masses)
â”œâ”€â”€ pl_eqt: DOUBLE          # Equilibrium temperature (K)
â”œâ”€â”€ st_teff: DOUBLE         # Stellar temperature (K)
â”œâ”€â”€ st_rad: DOUBLE          # Stellar radius (Solar radii)
â”œâ”€â”€ st_mass: DOUBLE         # Stellar mass (Solar masses)
â””â”€â”€ sy_dist: DOUBLE         # System distance (parsecs)
```

### Enriched Data Schema

```
exoplanets_enriched.parquet
â”œâ”€â”€ [All raw columns]
â”œâ”€â”€ is_earth_size: INT          # 1 if 0.5-2.0 Earth radii
â”œâ”€â”€ is_habitable_temp: INT      # 1 if 200-350K
â”œâ”€â”€ is_sun_like_star: INT       # 1 if 4000-7000K
â”œâ”€â”€ is_nearby: INT              # 1 if <1000 parsecs
â”œâ”€â”€ habitability_score: INT     # Sum of above (0-4)
â””â”€â”€ planet_category: STRING     # Rocky/Super-Earth/Neptune/Jupiter
```

---

## âš™ï¸ Processing Pipeline

### Stage 1: Data Ingestion
- **Tool**: Python + Requests library
- **Source**: NASA Exoplanet Archive TAP API
- **Format**: CSV
- **Size**: ~5MB (5,600+ records)

### Stage 2: Data Upload
- **Tool**: HDFS CLI
- **Destination**: `/user/exoplanets/raw/`
- **Replication**: 3x

### Stage 3: Spark Processing
- **Executor Memory**: 2G
- **Executor Cores**: 2
- **Number of Executors**: 2-4
- **Operations**:
  - Data validation & cleaning
  - Feature engineering
  - Aggregations
  - Parquet conversion

### Stage 4: Hive Table Creation
- **Table Type**: External
- **Storage Format**: Parquet
- **Partitioning**: None (small dataset)
- **Compression**: Snappy

### Stage 5: Analysis
- **Tool**: Hive SQL + Python
- **Queries**: Ad-hoc analytical queries
- **Output**: Insights & visualizations

---

## ğŸ”§ Technology Choices

### Why Hadoop?
- **Distributed storage**: Scalable for large datasets
- **Fault tolerance**: Data replication
- **Industry standard**: Relevant for real-world scenarios

### Why Spark?
- **In-memory processing**: 100x faster than MapReduce
- **DataFrame API**: Intuitive data manipulation
- **Unified engine**: Batch + streaming capabilities

### Why Hive?
- **SQL interface**: Familiar query language
- **Data warehouse**: Structured analytical queries
- **Integration**: Works seamlessly with HDFS & Spark

### Why Docker?
- **Reproducibility**: Same environment everywhere
- **Quick setup**: No complex installation
- **Isolation**: Clean development environment

### Why Parquet?
- **Columnar storage**: Efficient for analytics
- **Compression**: 80% smaller than CSV
- **Performance**: Fast read/write operations

---

## ğŸš€ Performance Considerations

### Optimization Strategies

1. **Data Format**
   - Use Parquet instead of CSV (4-5x faster queries)
   - Enable Snappy compression

2. **Spark Configuration**
   - Tune executor memory based on data size
   - Use appropriate number of partitions
   - Cache frequently accessed DataFrames

3. **Hive Optimization**
   - Use external tables for flexibility
   - Avoid complex JOINs on large tables
   - Use EXPLAIN for query optimization

4. **Storage**
   - HDFS block size: 128MB (default)
   - Replication factor: 3 for small clusters

---

## ğŸ“Š Scalability

### Current Setup (Small Dataset)
- **Data size**: ~5MB CSV â†’ ~1MB Parquet
- **Records**: 5,600+
- **Processing time**: <1 minute

### Future Scalability
This architecture scales to:
- **Millions of records**: Add more DataNodes & Workers
- **Terabytes of data**: Increase cluster size
- **Complex analytics**: Add more processing nodes

---

## ğŸ” Security Considerations

For production deployment:
- Enable Kerberos authentication
- Configure HDFS permissions
- Use SSL/TLS for data in transit
- Implement role-based access control (RBAC)

---

## ğŸ“ Best Practices Applied

âœ… **Separation of concerns**: Each layer has single responsibility  
âœ… **Containerization**: Easy deployment & reproducibility  
âœ… **Version control**: All code in Git  
âœ… **Documentation**: Comprehensive architecture docs  
âœ… **Data formats**: Use efficient formats (Parquet)  
âœ… **Idempotency**: Scripts can be re-run safely  
âœ… **Logging**: Proper error handling & logging  
âœ… **Testing**: Data validation at each stage  

---

## ğŸ”— Related Documentation

- [Data Dictionary](data-dictionary.md)
- [Analysis Insights](insights.md)
- [Main README](../README.md)

---

*Last updated: 2025-10-20*
